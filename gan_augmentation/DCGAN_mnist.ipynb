{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "### Load and prepare the dataset\n",
    "\n",
    "You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "a4fYMGxGhrna",
    "outputId": "b21cb89a-3ed0-4d42-de32-11bc14c1461d"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]]],\n",
       "\n",
       "\n",
       "       [[[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]]],\n",
       "\n",
       "\n",
       "       [[[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]]],\n",
       "\n",
       "\n",
       "       [[[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]]],\n",
       "\n",
       "\n",
       "       [[[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]],\n",
       "\n",
       "        [[-1.],\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         ...,\n",
       "         [-1.],\n",
       "         [-1.],\n",
       "         [-1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Create the models\n",
    "\n",
    "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "### The Generator\n",
    "\n",
    "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWgG09LCSJl"
   },
   "source": [
    "Use the (as yet untrained) generator to create an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "gl7jcC7TdPTG",
    "outputId": "5792f54e-c677-4c11-ebca-05946d9e425d"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12544)             1254400   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1183fed6d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "### The Discriminator\n",
    "\n",
    "The discriminator is a CNN-based image classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhPneagzCaQv"
   },
   "source": [
    "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6273      \n",
      "=================================================================\n",
      "Total params: 212,865\n",
      "Trainable params: 212,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gDkA05NE6QMs",
    "outputId": "9a3844cb-2c43-4485-eaad-32d4cef6de5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.0005387]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss and optimizers\n",
    "\n",
    "Define loss functions and optimizers for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "psQfmXxYKU3X"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "### Discriminator loss\n",
    "\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "### Generator loss\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "The discriminator and the generator optimizers are different since we will train two networks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "### Save checkpoints\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Define the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 10\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jylSonrqSWfi"
   },
   "source": [
    "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFF7Hk3XdeW"
   },
   "source": [
    "**Generate and save images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(28,28))\n",
    "\n",
    "    a = 0\n",
    "    for i in range(predictions.shape[0]):\n",
    "    #       plt.plot(1, 1, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.savefig('/media/rajarshi/Projects/PROJECTS/PRODUCTS/Research/aug/{:04d}.png'.format(i))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Train the model\n",
    "Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
    "\n",
    "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Ly3UN0SLLY2l",
    "outputId": "e54171ba-dc08-45c4-c079-daa18370c3ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "Restore the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhXsd0srPo8c"
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
