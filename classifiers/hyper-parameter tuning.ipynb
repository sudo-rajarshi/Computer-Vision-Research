{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating neural network architecture\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "# for image processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# for dataset management\n",
    "import os, shutil\n",
    "\n",
    "# for time management\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices != []:\n",
    "    print(\"Using GPU\")\n",
    "    for i in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(i, True)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzGbeafozkGs"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = str(input(\"Path where 'classify train' directory belongs: \"))\n",
    "classify_train = os.path.join(root_dir, 'classify train')\n",
    "\n",
    "train_directory = os.path.join(classify_train, 'training')\n",
    "validation_directory = os.path.join(classify_train, 'validation')\n",
    "test_directory = os.path.join(classify_train, 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pnXEx3e0SY_"
   },
   "source": [
    "# Analyze Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOA4r9g-2QBG"
   },
   "source": [
    "### Get image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J_owMAp0TVM"
   },
   "outputs": [],
   "source": [
    "height_lis = []\n",
    "width_lis = []\n",
    "\n",
    "total = sum([len(files) for r, d, files in os.walk(classify_train)])\n",
    "\n",
    "with tqdm(total=total) as pbar:\n",
    "    for splitname in os.listdir(classify_train):\n",
    "        split_dir = os.path.join(classify_train, splitname)\n",
    "        for class_name in os.listdir(split_dir):\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                img = cv2.imread(file_path)\n",
    "\n",
    "                height_lis.append(img.shape[0])\n",
    "                width_lis.append(img.shape[1])\n",
    "\n",
    "                pbar.set_description(\"Progress\")\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByvybQQY0dQe"
   },
   "outputs": [],
   "source": [
    "avg_height = int(np.mean(height_lis))\n",
    "avg_width = int(np.mean(width_lis))\n",
    "\n",
    "smallest_height = int(min(height_lis))\n",
    "smallest_width = int(min(width_lis))\n",
    "\n",
    "print('Average height & width respectively: {}, {}'.format(avg_height, avg_width))\n",
    "print('Smallest height & width respectively: {}, {}'.format(smallest_height, smallest_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK1t8TMh1ihd"
   },
   "source": [
    "### Resize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ6kG3160exX"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "total = sum([len(files) for r, d, files in os.walk(classify_train)])\n",
    "\n",
    "with tqdm(total=total) as pbar:\n",
    "    for splitname in os.listdir(classify_train):\n",
    "        split_dir = os.path.join(classify_train, splitname)\n",
    "        for class_name in os.listdir(split_dir):\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                img = cv2.imread(file_path)\n",
    "                res = cv2.resize(img, (128, 128))\n",
    "                cv2.imwrite(file_path, res)\n",
    "            \n",
    "                pbar.set_description(\"Progress\")\n",
    "                pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi5aeVdf0nD1"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEhGM2Mf0jrW"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "steps = 1 # change steps to 1 to apply exponential decay\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return learning_rate * (0.1 ** int(epoch / steps))\n",
    "\n",
    "\n",
    "callback = [tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose = 1),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor = 'loss', min_delta = 0.001, patience = 10, verbose = 1, mode = \"min\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = len(os.listdir(train_directory))\n",
    "\n",
    "if class_no <= 2:\n",
    "    class_mode = 'binary'\n",
    "    output_activation = 'sigmoid'\n",
    "    output_neurons = 1\n",
    "else:\n",
    "    class_mode = 'categorical'\n",
    "    output_activation = 'softmax'\n",
    "    output_neurons = class_no\n",
    "   \n",
    "target_size = res.shape[0:2]\n",
    "dim = res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EUR7sE87_73"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_layers = int(input('Conv2d with activation + Max-pool + Dropout for feature extraction = 1 feature extraction layer \\nHow many of such feature extraction layers you want to use ? '))    \n",
    "no_conv = int(input('How many conv2d layers you want to use in each feature extraction layer ? '))\n",
    "no_filters = int(input('Put no. of filters in 1st conv2d layer: '))\n",
    "size_filter = int(input('Enter size of filter (width or height): '))\n",
    "f_dropout = int(input('Enter dropout rate for feature extraction: '))/100\n",
    "\n",
    "no_d_layers = int(input('Dense with activation + Dropout for desnse layer = 1 dense layer \\nHow many of such dense layers you want to use ? '))\n",
    "d_neurons = int(input('Enter no.of neurons you want to use in 1st dense layer: '))\n",
    "d_dropout = int(input('Enter dropout rate for dense layer: '))/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't put user inputs inside the function below as it'll be called multiple times inside a loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8uVqhyq7_nd"
   },
   "outputs": [],
   "source": [
    "def Custom_Model():\n",
    "    \n",
    "    model = Sequential(name = 'CUSTOM')\n",
    "    \n",
    "    \n",
    "    # feature extraction\n",
    "    m, n = 0, 0 # m = increamental factor of no. of filters, # n = total no. of filters in convolution layer\n",
    "    for l in range(no_layers):\n",
    "        m = 2**l  \n",
    "        n = no_filters*m \n",
    "        for i in range(no_conv):\n",
    "            model.add(Conv2D(n, \n",
    "                             (size_filter,size_filter), \n",
    "                             kernel_regularizer=l2(lambd), \n",
    "                             bias_regularizer=l2(lambd),\n",
    "                             padding = 'same', \n",
    "                             input_shape = dim))\n",
    "            model.add(LeakyReLU())\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(f_dropout))\n",
    "    \n",
    "    \n",
    "    # flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # dense layer\n",
    "    m, n = 0, 0\n",
    "    for d in range(no_d_layers):\n",
    "        m = 2**d\n",
    "        n = d_neurons//m\n",
    "        model.add(Dense(n, kernel_regularizer=l2(lambd), bias_regularizer=l2(lambd)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(d_dropout))\n",
    "    model.add(Dense(output_neurons, output_activation))\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "\n",
    "lambd_list = []\n",
    "lambd_no = int(input('Using how many lambdas you want to train with ? '))\n",
    "print('Enter {} lambda value/values consecutively:'.format(lambd_no))\n",
    "for i in range(lambd_no): \n",
    "    lambd = float(input('Enter lambda value: '))\n",
    "    lambd_list.append(lambd)\n",
    "\n",
    "batch_list = []\n",
    "batch_no = int(input(\"Enter how many batches you want to use: \"))\n",
    "print('Enter {} batch no. consecutively:'.format(batch_no))\n",
    "for j in range(batch_no):\n",
    "    b_size = int(input('Enter batch size: '))\n",
    "    batch_list.append(b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over batch sizes\n",
    "for batch_size in batch_list: \n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    train_generator = train_datagen.flow_from_directory(train_directory,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = class_mode,\n",
    "                                                        target_size = target_size)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    validation_generator = val_datagen.flow_from_directory(validation_directory,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = class_mode,\n",
    "                                                        target_size = target_size)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(test_directory,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = class_mode,\n",
    "                                                        target_size = target_size)\n",
    "    \n",
    "    # iterate over lambdas\n",
    "    for lambd in lambd_list:\n",
    "        model = Custom_Model()\n",
    "        \n",
    "        # COMPILE\n",
    "        loss = 'categorical_crossentropy'\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        model.compile(loss = loss,\n",
    "                    optimizer = optimizer,\n",
    "                    metrics=['accuracy',\n",
    "                    tf.keras.metrics.TruePositives(), \n",
    "                    tf.keras.metrics.TrueNegatives(), \n",
    "                    tf.keras.metrics.FalsePositives(),\n",
    "                    tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "        print ('\\n************ for lambda = {}************\\n'.format(lambd))\n",
    "\n",
    "        # FIT\n",
    "        history = model.fit(train_generator,\n",
    "                        epochs = epoch,\n",
    "                        verbose = 1,\n",
    "                        callbacks = callback,\n",
    "                        validation_data = validation_generator,\n",
    "                        shuffle = True)\n",
    "\n",
    "        # PLOT\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(len(acc))\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Accuracy vs Epochs\n",
    "        plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title('Training and validation accuracy vs Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Loss vs Epochs\n",
    "        plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "        plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title('Training and validation loss vs Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # ACCURACIES\n",
    "        print(\"Training accuracy: {}\".format(model.evaluate(train_generator, verbose=0)[1]))\n",
    "        print(\"Validation accuracy: {}\".format(model.evaluate(validation_generator, verbose=0)[1]))\n",
    "        print(\"Blind test accuracy: {}\".format(model.evaluate(test_generator, verbose=0)[1]))\n",
    "        tp = int(model.evaluate(test_generator, verbose=0)[2])\n",
    "        tn = int(model.evaluate(test_generator, verbose=0)[3])\n",
    "        fp = int(model.evaluate(test_generator, verbose=0)[4])\n",
    "        fn = int(model.evaluate(test_generator, verbose=0)[5])\n",
    "        sensitivity = (tp/(tp+fn))*100\n",
    "        specificity = (tn/(tn+fp))*100\n",
    "        print(\"Sensitivity: {}\".format(sensitivity))\n",
    "        print(\"Specificity: {}\".format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Respiratory.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
