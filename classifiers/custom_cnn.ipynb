{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib\n",
    "import glob\n",
    "import operator\n",
    "import psutil\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices != []:\n",
    "    print(\"Using GPU\")\n",
    "    for i in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(i, True)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = str(input(\"Path where 'classify train' directory belongs: \"))\n",
    "classify_train = os.path.join(root_dir, 'classify train')\n",
    "\n",
    "train_directory = os.path.join(classify_train, 'training')\n",
    "validation_directory = os.path.join(classify_train, 'validation')\n",
    "test_directory = os.path.join(classify_train, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory, validation_directory, test_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter setting-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = float(input(\"Enter the initial learning rate: \"))\n",
    "epoch = int(input(\"Enter the maximum number of epochs: \"))\n",
    "batch_size = int(input(\"Enter batch size: \"))\n",
    "lambd = float(input(\"Enter lambda for L2 regularization: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristics folder\n",
    "Creating a seperate folder where 'classify train' belongs to store model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_name = str(input(\"Enter name of the characteristics folder: \"))\n",
    "\n",
    "char = os.path.join(root_dir, char_name)\n",
    "\n",
    "if not os.path.exists(char):\n",
    "    os.mkdir(char)\n",
    "else:\n",
    "    replace = str(input(\"Folder already exists ! Do you want to replace it ?(Y/N) \"))\n",
    "    if replace.upper() == 'Y':      \n",
    "        shutil.rmtree(char)\n",
    "        os.mkdir(char)\n",
    "    elif replace.upper() == 'N':\n",
    "        print('\\nThe following folders already exist:')\n",
    "        for i in os.listdir(root_dir): \n",
    "            print(i)\n",
    "        char_name = str(input(\"\\nEnter a new name of the characteristics folder: \"))\n",
    "        char = os.path.join(root_dir, char_name)\n",
    "        if not os.path.exists(char):\n",
    "            os.mkdir(char)\n",
    "        else:\n",
    "            print(f\"{char_name} replaced\")\n",
    "            shutil.rmtree(char)\n",
    "            os.mkdir(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1 # change steps to 1 to apply exponential decay\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return learning_rate * (0.1 ** int(epoch / steps))\n",
    "    \n",
    "best_model_address = os.path.join(char, 'best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = int(input(\"Press 1 to monitor Validation Accuracy\\nPress 2 to monitor Validation Loss\\nPress 3 to monitor Training Accuracy\\nPress 4 to monitor Training Loss\\n\"))\n",
    "patience = int(input('Enter number of epochs that will produce monitored quantity with no improvement after which training will be stopped: '))\n",
    "\n",
    "\n",
    "if monitor == 1:\n",
    "    metric = 'val_accuracy'\n",
    "    mode = 'max'\n",
    "    print(\"\\nMONITORING VALIDATION ACCURACY..........\\n\")\n",
    "\n",
    "elif monitor == 2:\n",
    "    metric = 'val_loss'\n",
    "    mode = 'min'\n",
    "    print(\"\\nMONITORING VALIDATION LOSS..........\\n\")\n",
    "\n",
    "elif monitor == 3:\n",
    "    metric = 'accuracy'\n",
    "    mode = 'max'\n",
    "    print(\"\\nMONITORING TRAINING ACCURACY..........\\n\")\n",
    "\n",
    "elif monitor == 4:\n",
    "    metric = 'loss'\n",
    "    mode = 'min'\n",
    "    print(\"\\nMONITORING TRAINING LOSS..........\\n\")\n",
    "\n",
    "callback = [keras.callbacks.LearningRateScheduler(lr_schedule, verbose = 1),\n",
    "            keras.callbacks.EarlyStopping(monitor = metric, min_delta = 0.001, patience = patience, verbose=1, mode = mode, restore_best_weights = True),\n",
    "            keras.callbacks.ModelCheckpoint(best_model_address, monitor = metric, verbose=1, save_best_only=True, save_weights_only=False, mode = mode)]\n",
    "\n",
    "print(f\"\\nTraining will stop if {metric} doesn't show any improvements for {patience} epcohs\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_Model():\n",
    "    no_layers = int(input('Conv2d with activation + Max-pool + Dropout for feature extraction = 1 feature extraction layer \\nHow many of such feature extraction layers you want to use ? '))    \n",
    "    no_conv = int(input('How many conv2d layers you want to use in each feature extraction layer ? '))\n",
    "    no_filters = int(input('Put no. of filters in 1st conv2d layer: '))\n",
    "    size_filter = int(input('Enter size of filter (width or height): '))\n",
    "    f_dropout = int(input('Enter dropout rate for feature extraction: '))/100\n",
    "    \n",
    "    no_d_layers = int(input('Dense with activation + Dropout for desnse layer = 1 dense layer \\nHow many of such dense layers you want to use ? '))\n",
    "    d_neurons = int(input('Enter no.of neurons you want to use in 1st dense layer: '))\n",
    "    d_dropout = int(input('Enter dropout rate for dense layer: '))/100\n",
    "            \n",
    "    \n",
    "    model = Sequential(name = 'CUSTOM')\n",
    "    \n",
    "    \n",
    "    # feature extraction\n",
    "    m, n = 0, 0 # m = increamental factor of no. of filters, # n = total no. of filters in convolution layer\n",
    "    for l in range(no_layers):\n",
    "        m = 2**l  \n",
    "        n = no_filters*m \n",
    "        for i in range(no_conv):\n",
    "            model.add(Conv2D(n, \n",
    "                             (size_filter,size_filter), \n",
    "                             kernel_regularizer=l2(lambd), \n",
    "                             bias_regularizer=l2(lambd),\n",
    "                             padding = 'same', \n",
    "                             input_shape = dim))\n",
    "            model.add(LeakyReLU())\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(f_dropout))\n",
    "    \n",
    "    \n",
    "    # flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # dense layer\n",
    "    m, n = 0, 0\n",
    "    for d in range(no_d_layers):\n",
    "        m = 2**d\n",
    "        n = d_neurons//m\n",
    "        model.add(Dense(n, kernel_regularizer=l2(lambd), bias_regularizer=l2(lambd)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(d_dropout))\n",
    "    model.add(Dense(output_neurons, output_activation))\n",
    "\n",
    "    \n",
    "    return model, size_filter, f_dropout, d_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = len(os.listdir(train_directory))\n",
    "\n",
    "print(\"This is a \" + str(class_no) + \"-Class Classification\")\n",
    "\n",
    "if class_no <= 2:\n",
    "    class_mode = 'binary'\n",
    "    output_activation = 'sigmoid'\n",
    "    output_neurons = 1\n",
    "    losses = 'binary_crossentropy'\n",
    "\n",
    "else:\n",
    "    class_mode = 'categorical'\n",
    "    output_activation = 'softmax'\n",
    "    output_neurons = class_no\n",
    "    losses = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_selection():\n",
    "    print(\"\\nSelect a optimizer which will reduce the loss of the model.\\n\")\n",
    "\n",
    "    optimizer_select = int(input(\"Press 1 to select Stochastic Gradient Descent\\nPress 2 to select RMSprop\\nPress 3 to select Adagrad\\nPress 4 to select Adadelta\\nPress 5 to select Adam\\nPress 6 to select Adamax\\nPress 7 to select Nadam\\n\"))\n",
    "\n",
    "    if optimizer_select == 1:\n",
    "        optimizer = SGD(lr = learning_rate, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "    elif optimizer_select == 2:\n",
    "        optimizer = RMSprop(learning_rate, rho = 0.9)\n",
    "\n",
    "    elif optimizer_select == 3:\n",
    "        optimizer = Adagrad(learning_rate)\n",
    "\n",
    "    elif optimizer_select == 4:\n",
    "        optimizer = Adadelta(learning_rate, rho = 0.95)\n",
    "\n",
    "    elif optimizer_select == 5:\n",
    "        optimizer = Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "    elif optimizer_select == 6:\n",
    "        optimizer = Adamax(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n",
    "\n",
    "    elif optimizer_select == 7:\n",
    "        optimizer = Nadam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n",
    "   \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = int(input(\"Image Dimension(H or W): \"))\n",
    "w = h\n",
    "color = int(input(\"Press 1 for RGB \\nPress 2 for Grayscale \"))\n",
    "if color == 1:\n",
    "    color_mode = 'rgb'\n",
    "    dim = (h,w,3)\n",
    "elif color == 2:\n",
    "    color_mode = 'grayscale'\n",
    "    dim = (h,w,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "train_generator = train_datagen.flow_from_directory(train_directory,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = class_mode,\n",
    "                                                    color_mode = color_mode,\n",
    "                                                    target_size = (h,w),\n",
    "                                                    shuffle=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_directory,\n",
    "                                                              batch_size = batch_size,\n",
    "                                                              class_mode = class_mode,\n",
    "                                                              color_mode = color_mode,\n",
    "                                                              target_size = (h,w),\n",
    "                                                              shuffle=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
    "test_generator = test_datagen.flow_from_directory(test_directory,\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  class_mode = class_mode,\n",
    "                                                  color_mode = color_mode,\n",
    "                                                  target_size = (h,w),\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, size_filter, f_dropout, d_dropout = Custom_Model()\n",
    "model.compile(optimizer = optimizer, loss = losses, metrics = ['accuracy', \n",
    "                                                               tf.keras.metrics.Precision(), \n",
    "                                                               tf.keras.metrics.Recall(), \n",
    "                                                               tf.keras.metrics.TruePositives(), \n",
    "                                                               tf.keras.metrics.TrueNegatives(), \n",
    "                                                               tf.keras.metrics.FalsePositives(),\n",
    "                                                               tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    start = time.time()\n",
    "    history = model.fit(train_generator,\n",
    "                        epochs = epoch,\n",
    "                        verbose = 1,\n",
    "                        callbacks = callback,\n",
    "                        validation_data = validation_generator,\n",
    "                        shuffle=True)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "\n",
    "train_score = model.evaluate(train_generator)\n",
    "val_score = model.evaluate(validation_generator)\n",
    "test_score = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execution Time: {} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot characteristic curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick = []\n",
    "xe_tick = []\n",
    "i = 0\n",
    "for i in epochs:\n",
    "    if i%8 == 0:\n",
    "        x_tick.append(i)\n",
    "\n",
    "x_tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(x_tick)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Training and validation accuracy vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "fig_name_eps = \"accuracy.eps\"\n",
    "fig_name_jpg = \"accuracy.jpg\"\n",
    "\n",
    "plt.savefig(os.path.join(char, fig_name_eps))\n",
    "plt.savefig(os.path.join(char, fig_name_jpg))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(x_tick)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Training and validation loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "fig_name_eps = \"loss.eps\"\n",
    "fig_name_jpg = \"loss.jpg\"\n",
    "\n",
    "plt.savefig(os.path.join(char, fig_name_eps))\n",
    "plt.savefig(os.path.join(char, fig_name_jpg))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(x_tick)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Training and validation loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "fig_name_eps = \"char.eps\"\n",
    "fig_name_jpg = \"char.jpg\"\n",
    "\n",
    "plt.savefig(os.path.join(char, fig_name_eps))\n",
    "plt.savefig(os.path.join(char, fig_name_jpg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = train_score[1]*100\n",
    "validation_accuracy = val_score[1]*100\n",
    "test_accuracy = test_score[1]*100\n",
    "\n",
    "print(\"The training accuracy is: \" + str(training_accuracy) + ' %')\n",
    "print(\"The validation accuracy is: \" + str(validation_accuracy) + ' %')\n",
    "print(\"The test accuracy is: \" + str(test_accuracy) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = test_score[1]*100\n",
    "test_precision = test_score[2]*100\n",
    "test_recall = test_score[3]*100\n",
    "tp = int(test_score[4])\n",
    "tn = int(test_score[5])\n",
    "fp = int(test_score[6])\n",
    "fn = int(test_score[7])\n",
    "\n",
    "f1 = 2*((test_precision*test_recall)/(test_precision+test_recall))\n",
    "sensitivity = (tp/(tp+fn))*100\n",
    "specificity = (tn/(tn+fp))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Accuracy: {}\".format(test_accuracy))\n",
    "print(\"Test Precision: {}\".format(test_precision))\n",
    "print(\"Test Recall: {}\".format(test_recall))\n",
    "print(\"True Positive: {}\".format(tp))\n",
    "print(\"Test Negetive: {}\".format(tn))\n",
    "print(\"False Positive: {}\".format(fp))\n",
    "print(\"False Negetive: {}\".format(fn))\n",
    "print(\"Sensitivity: {}\".format(sensitivity))\n",
    "print(\"Specificity: {}\".format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(test_directory, test_generator, class_no, best_model_address, dim):\n",
    "    test_class_list = []\n",
    "    for test_name in os.listdir(test_directory):\n",
    "        test = os.path.join(test_directory, test_name)\n",
    "        test_class_list.append(test)\n",
    "    test_class_list.sort()\n",
    "    \n",
    "    y_true = test_generator.classes\n",
    "    labels = test_generator.class_indices\n",
    "    \n",
    "    y_pred = []\n",
    "    tot = len(os.listdir(test_class_list[1]))*class_no\n",
    "\n",
    "    best_model = load_model(best_model_address)\n",
    "    \n",
    "    with tqdm(total=tot) as pbar:\n",
    "        for i in range(class_no):\n",
    "            for filename in os.listdir(test_class_list[i]):\n",
    "                file = os.path.join(test_class_list[i], filename)\n",
    "                img = cv2.imread(file)\n",
    "                res = cv2.resize(img, (dim[0], dim[1]))\n",
    "                normed = res / 255.0\n",
    "                im_arr = normed.reshape(1, dim[0], dim[1], dim[2])\n",
    "\n",
    "                pred = best_model.predict(im_arr)\n",
    "                pred_categorical = keras.utils.to_categorical(pred)\n",
    "\n",
    "                if class_no >= 2:\n",
    "                    max_pred = np.argmax(pred)\n",
    "                else:\n",
    "                    max_pred = np.argmax(pred_categorical)\n",
    "\n",
    "                y_pred.append(max_pred)\n",
    "\n",
    "                pbar.set_description(\"Progress\")\n",
    "                pbar.update()\n",
    "                \n",
    "    return y_true, y_pred, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix \n",
    "    \n",
    "    print(\"Calculating CLASSIFICATION REPORT: \")\n",
    "    classification_reports = classification_report(y_true, y_pred, target_names=labels)\n",
    "    print(classification_reports)\n",
    "\n",
    "    print(\"\\nCalculating SENSITIVITY & SPECIFICITY..........:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    total = sum(sum(cm))\n",
    "    sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "    specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "    print(\"sensitivity = {:.4f}\".format(sensitivity))\n",
    "    print(\"specificity = {:.4f}\".format(specificity))\n",
    "    \n",
    "    return cm, classification_reports, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, labels = pred(test_directory, test_generator, class_no, best_model_address, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, classification_reports, sensitivity, specificity = report(y_true, y_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(cm, labels, char):\n",
    "    fig, ax = plot_confusion_matrix(conf_mat=cm,\n",
    "                                colorbar=True,\n",
    "                                show_absolute=True,\n",
    "                                class_names=labels,\n",
    "                                show_normed=True)\n",
    "\n",
    "    plt.savefig(os.path.join(char, 'confusion-matrix.eps'))\n",
    "    plt.savefig(os.path.join(char, 'confusion-matrix.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat(cm, labels, char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "readme_name_text = \"readme.txt\"\n",
    "print(f\"Please read the text file named {readme_name_text} for detailed information of the model.\")\n",
    "\n",
    "completeName_txt = os.path.join(char, readme_name_text) \n",
    "\n",
    "readme = open(completeName_txt, \"w\")\n",
    "\n",
    "if len(os.listdir(train_directory)) > 2:\n",
    "    readme.write(f\"This is a {len(os.listdir(train_directory))}-class CLASSIFICATION\")\n",
    "else:\n",
    "    readme.write(\"This is a BINARY CLASSIFICATION\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--HYPERPARAMETERS--\\n\")\n",
    "readme.write(f\"\\nInitial Learning Rate = {learning_rate}\")\n",
    "readme.write(f\"\\nNo. of epochs = {len(acc)}\")\n",
    "readme.write(f\"\\nBatch Size = {batch_size}\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--MODEL-PARAMETERS--\")\n",
    "readme.write(f\"\\nDropout for feature extraction = {(int(f_dropout*100))} %\")\n",
    "readme.write(f\"\\nDropout for dense layer = {(int(d_dropout*100))} %\")\n",
    "readme.write(f\"\\nOptimizer = {optimizer}\\n\\n\")\n",
    "\n",
    "\n",
    "readme.write(\"Trained on a Custom Prebuilt Model\\n\")\n",
    "readme.write(f\"\\nFilter size = {size_filter}x{size_filter}\\n\\n\")\n",
    "with redirect_stdout(readme):\n",
    "    model.summary()\n",
    "        \n",
    "    \n",
    "readme.write(\"\\n\\n--MODEL-PERFORMANCE--\")\n",
    "readme.write(f\"\\nTest Accuracy = {test_accuracy} %\")\n",
    "readme.write(f\"\\nTest Precision = {test_precision} %\")\n",
    "readme.write(f\"\\nTest Recall = {test_recall} %\")\n",
    "readme.write(f\"\\nTrue Positive = {tp}\")\n",
    "readme.write(f\"\\nTrue Negetive = {tn}\")\n",
    "readme.write(f\"\\nFalse Positive = {fp}\")\n",
    "readme.write(f\"\\nFalse Negetive = {fn}\")\n",
    "readme.write(f\"\\nSensitivity = {sensitivity}\")\n",
    "readme.write(f\"\\nSpecificity = {specificity}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--MODEL-CHARACTERISTICS--\")\n",
    "readme.write(f\"\\nacc = {acc}\")\n",
    "readme.write(f\"\\n\\nval_acc = {val_acc}\")\n",
    "readme.write(f\"\\n\\nloss = {loss}\")\n",
    "readme.write(f\"\\n\\nval_loss = {val_loss}\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--Classification Report--\\n\")\n",
    "readme.write(classification_reports)\n",
    "\n",
    "readme.write(f\"\\nSensitivity = {sensitivity*100} %\")\n",
    "readme.write(f\"\\nSpecificity = {specificity*100} %\")\n",
    "\n",
    "\n",
    "readme.write(f\"\\nExecution Time: {duration} seconds\")\n",
    "\n",
    "readme.write(\"\\n\\nCreated using Self-Regulated Image Classifier using Convolution Neural Network\")\n",
    "\n",
    "readme.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
