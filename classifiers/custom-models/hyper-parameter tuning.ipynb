{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating neural network architecture\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "# for image processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# for dataset management\n",
    "import os, shutil\n",
    "\n",
    "# for time management\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices != []:\n",
    "    print(\"Using GPU\")\n",
    "    for i in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(i, True)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzGbeafozkGs"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = str(input(\"Path where 'classify train' directory belongs: \"))\n",
    "classify_train = os.path.join(root_dir, 'classify train')\n",
    "\n",
    "train_directory = os.path.join(classify_train, 'training')\n",
    "validation_directory = os.path.join(classify_train, 'validation')\n",
    "test_directory = os.path.join(classify_train, 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi5aeVdf0nD1"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEhGM2Mf0jrW"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "steps = 1 # change steps to 1 to apply exponential decay\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return learning_rate * (0.1 ** int(epoch / steps))\n",
    "\n",
    "\n",
    "callback = [tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose = 1),\n",
    "            tf.keras.callbacks.EarlyStopping(monitor = 'loss', min_delta = 0.001, patience = 10, verbose = 1, mode = \"min\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = len(os.listdir(train_directory))\n",
    "\n",
    "print(\"This is a \" + str(class_no) + \"-Class Classification\")\n",
    "\n",
    "if class_no <= 2:\n",
    "    class_mode = 'binary'\n",
    "    output_activation = 'sigmoid'\n",
    "    output_neurons = 1\n",
    "    losses = 'binary_crossentropy'\n",
    "\n",
    "else:\n",
    "    class_mode = 'categorical'\n",
    "    output_activation = 'softmax'\n",
    "    output_neurons = class_no\n",
    "    losses = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = int(input(\"Image Dimension(H or W): \"))\n",
    "w = h\n",
    "target_size = (h,w)\n",
    "color = int(input(\"Press 1 for RGB \\nPress 2 for Grayscale \"))\n",
    "if color == 1:\n",
    "    color_mode = 'rgb'\n",
    "    dim = (h,w,3)\n",
    "elif color == 2:\n",
    "    color_mode = 'grayscale'\n",
    "    dim = (h,w,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EUR7sE87_73"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't put user inputs inside the function below as it'll be called multiple times inside a loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8uVqhyq7_nd"
   },
   "outputs": [],
   "source": [
    "def MobileNet():\n",
    "    print(\"\\nTRAINING ON MobileNet MODEL:-\")\n",
    "\n",
    "    full_model = tf.keras.applications.MobileNet(input_shape = (224,224,3), weights = 'imagenet', include_top = True)\n",
    "    full_model.summary()\n",
    "    \n",
    "    no_d_layers = 1\n",
    "    d_neurons = 128\n",
    "    d_dropout = 0.2\n",
    "    \n",
    "    base_model = tf.keras.applications.MobileNet(input_shape = dim, weights = 'imagenet', include_top = False)\n",
    "\n",
    "    x = base_model.output\n",
    "    \n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    \n",
    "    m, n = 0, 0\n",
    "    for d in range(no_d_layers):\n",
    "        m = 2**d\n",
    "        n = d_neurons//m\n",
    "        x = Dense(n, kernel_regularizer=l2(lambd), bias_regularizer=l2(lambd))(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dropout(d_dropout)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    predictions = Dense(output_neurons, activation = output_activation)(x)  \n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs=predictions)\n",
    "\n",
    "    train_base_model = \"Y\"\n",
    "    if train_base_model.upper() == 'Y':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = True\n",
    "    elif train_base_model.upper() == 'N':\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "\n",
    "lambd_list = []\n",
    "lambd_no = int(input('Using how many lambdas you want to train with ? '))\n",
    "print('Enter {} lambda value/values consecutively:'.format(lambd_no))\n",
    "for i in range(lambd_no): \n",
    "    lambd = float(input('Enter lambda value: '))\n",
    "    lambd_list.append(lambd)\n",
    "\n",
    "batch_list = []\n",
    "batch_no = int(input(\"Enter how many batches you want to use: \"))\n",
    "print('Enter {} batch no. consecutively:'.format(batch_no))\n",
    "for j in range(batch_no):\n",
    "    b_size = int(input('Enter batch size: '))\n",
    "    batch_list.append(b_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over batch sizes\n",
    "for batch_size in batch_list: \n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    train_generator = train_datagen.flow_from_directory(train_directory,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = class_mode,\n",
    "                                                        target_size = target_size)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    validation_generator = val_datagen.flow_from_directory(validation_directory,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = class_mode,\n",
    "                                                        target_size = target_size)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(test_directory,\n",
    "                                                        batch_size = batch_size,\n",
    "                                                        class_mode = class_mode,\n",
    "                                                        target_size = target_size)\n",
    "    \n",
    "    # iterate over lambdas\n",
    "    for l in lambd_list:\n",
    "        model = MobileNet()\n",
    "        \n",
    "        # COMPILE\n",
    "        loss = 'categorical_crossentropy'\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "        model.compile(loss = loss,\n",
    "                    optimizer = optimizer,\n",
    "                    metrics=['accuracy',\n",
    "                    tf.keras.metrics.TruePositives(), \n",
    "                    tf.keras.metrics.TrueNegatives(), \n",
    "                    tf.keras.metrics.FalsePositives(),\n",
    "                    tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "        print ('\\n************ for lambda = {}************\\n'.format(lambd))\n",
    "\n",
    "        # FIT\n",
    "        history = model.fit(train_generator,\n",
    "                        epochs = epoch,\n",
    "                        verbose = 1,\n",
    "                        callbacks = callback,\n",
    "                        validation_data = validation_generator,\n",
    "                        shuffle = True)\n",
    "\n",
    "        # PLOT\n",
    "        acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(len(acc))\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Accuracy vs Epochs\n",
    "        plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title('Training and validation accuracy vs Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Loss vs Epochs\n",
    "        plt.plot(epochs, loss, 'r', label=\"Training Loss\")\n",
    "        plt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title('Training and validation loss vs Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # ACCURACIES\n",
    "        print(\"Training accuracy: {}\".format(model.evaluate(train_generator, verbose=0)[1]))\n",
    "        print(\"Validation accuracy: {}\".format(model.evaluate(validation_generator, verbose=0)[1]))\n",
    "        print(\"Blind test accuracy: {}\".format(model.evaluate(test_generator, verbose=0)[1]))\n",
    "        tp = int(model.evaluate(test_generator, verbose=0)[2])\n",
    "        tn = int(model.evaluate(test_generator, verbose=0)[3])\n",
    "        fp = int(model.evaluate(test_generator, verbose=0)[4])\n",
    "        fn = int(model.evaluate(test_generator, verbose=0)[5])\n",
    "        sensitivity = (tp/(tp+fn))*100\n",
    "        specificity = (tn/(tn+fp))*100\n",
    "        print(\"Sensitivity: {}\".format(sensitivity))\n",
    "        print(\"Specificity: {}\".format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Respiratory.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
