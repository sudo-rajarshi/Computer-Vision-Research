{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib\n",
    "import glob\n",
    "import operator\n",
    "import psutil\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices()\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if physical_devices != []:\n",
    "    print(\"Using GPU\")\n",
    "    for i in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(i, True)\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = str(input(\"Path where 'classify train' directory belongs: \"))\n",
    "classify_train = os.path.join(root_dir, 'classify train')\n",
    "\n",
    "train_directory = os.path.join(classify_train, 'training')\n",
    "validation_directory = os.path.join(classify_train, 'validation')\n",
    "test_directory = os.path.join(classify_train, 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "valX = []\n",
    "testX = []\n",
    "\n",
    "trainY = []\n",
    "valY = []\n",
    "testY = []\n",
    "\n",
    "for class_name in os.listdir(train_directory):\n",
    "    class_dir = os.path.join(train_directory, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        x = cv2.imread(file_path)\n",
    "        trainX.append(x)\n",
    "        if 'normal' in file_path:\n",
    "            trainY.append(0)\n",
    "        elif 'pneumonia' in file_path:\n",
    "            trainY.append(1)\n",
    "        elif 'covid' in file_path:\n",
    "            trainY.append(2)\n",
    "\n",
    "\n",
    "for class_name in os.listdir(validation_directory):\n",
    "    class_dir = os.path.join(validation_directory, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        x = cv2.imread(file_path)\n",
    "        valX.append(x)  \n",
    "        if 'normal' in file_path:\n",
    "            valY.append(0)\n",
    "        elif 'pneumonia' in file_path:\n",
    "            valY.append(1)\n",
    "        elif 'covid' in file_path:\n",
    "            valY.append(2)\n",
    "\n",
    "        \n",
    "for class_name in os.listdir(test_directory):\n",
    "    class_dir = os.path.join(test_directory, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        x = cv2.imread(file_path)\n",
    "        testX.append(x)  \n",
    "        if 'normal' in file_path:\n",
    "            testY.append(0)\n",
    "        elif 'pneumonia' in file_path:\n",
    "            testY.append(1)\n",
    "        elif 'covid' in file_path:\n",
    "            testY.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.array(trainX)\n",
    "valX = np.array(valX)\n",
    "testX = np.array(testX)\n",
    "\n",
    "trainY = np.array(trainY)\n",
    "valY = np.array(valY)\n",
    "testY = np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valX.shape)\n",
    "print(valY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_norm = trainX.astype('float32')\n",
    "valX_norm = valX.astype('float32')\n",
    "testX_norm = testX.astype('float32')\n",
    "\n",
    "trainX_norm = trainX_norm / 255.0\n",
    "valX_norm = valX_norm / 255.0\n",
    "testX_norm = testX_norm / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX_norm.shape)\n",
    "print(valX_norm.shape)\n",
    "print(testX_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = to_categorical(trainY)\n",
    "valY = to_categorical(valY)\n",
    "testY = to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((trainX_norm, testX_norm), axis=0)\n",
    "y = np.concatenate((trainY, testY), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = float(input(\"Enter the initial learning rate: \"))\n",
    "epoch = int(input(\"Enter the maximum number of epochs: \"))\n",
    "batch_size = int(input(\"Enter batch size: \"))\n",
    "lambd = float(input(\"Enter lambda for L2 regularization: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_name = str(input(\"Enter name of the characteristics folder: \"))\n",
    "\n",
    "char = os.path.join(root_dir, char_name)\n",
    "\n",
    "if not os.path.exists(char):\n",
    "    os.mkdir(char)\n",
    "else:\n",
    "    replace = str(input(\"Folder already exists ! Do you want to replace it ?(Y/N) \"))\n",
    "    if replace.upper() == 'Y':      \n",
    "        shutil.rmtree(char)\n",
    "        os.mkdir(char)\n",
    "    elif replace.upper() == 'N':\n",
    "        print('\\nThe following folders already exist:')\n",
    "        for i in os.listdir(root_dir): \n",
    "            print(i)\n",
    "        char_name = str(input(\"\\nEnter a new name of the characteristics folder: \"))\n",
    "        char = os.path.join(root_dir, char_name)\n",
    "        if not os.path.exists(char):\n",
    "            os.mkdir(char)\n",
    "        else:\n",
    "            print(f\"{char_name} replaced\")\n",
    "            shutil.rmtree(char)\n",
    "            os.mkdir(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10 # change steps to 1 to apply exponential decay\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    return learning_rate * (0.1 ** int(epoch / steps))\n",
    "    \n",
    "best_model_address = os.path.join(char, 'best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = int(input(\"Press 1 to monitor Validation Accuracy\\nPress 2 to monitor Validation Loss\\nPress 3 to monitor Training Accuracy\\nPress 4 to monitor Training Loss\\n\"))\n",
    "patience = int(input('Enter number of epochs that will produce monitored quantity with no improvement after which training will be stopped: '))\n",
    "\n",
    "\n",
    "if monitor == 1:\n",
    "    metric = 'val_accuracy'\n",
    "    mode = 'max'\n",
    "    print(\"\\nMONITORING VALIDATION ACCURACY..........\\n\")\n",
    "\n",
    "elif monitor == 2:\n",
    "    metric = 'val_loss'\n",
    "    mode = 'min'\n",
    "    print(\"\\nMONITORING VALIDATION LOSS..........\\n\")\n",
    "\n",
    "elif monitor == 3:\n",
    "    metric = 'accuracy'\n",
    "    mode = 'max'\n",
    "    print(\"\\nMONITORING TRAINING ACCURACY..........\\n\")\n",
    "\n",
    "elif monitor == 4:\n",
    "    metric = 'loss'\n",
    "    mode = 'min'\n",
    "    print(\"\\nMONITORING TRAINING LOSS..........\\n\")\n",
    "\n",
    "callback = [keras.callbacks.LearningRateScheduler(lr_schedule, verbose = 1),\n",
    "            keras.callbacks.EarlyStopping(monitor = metric, min_delta = 0.001, patience = patience, verbose=1, mode = mode, restore_best_weights = True),\n",
    "            keras.callbacks.ModelCheckpoint(best_model_address, monitor = metric, verbose=1, save_best_only=True, save_weights_only=False, mode = mode)]\n",
    "\n",
    "print(f\"\\nTraining will stop if {metric} doesn't show any improvements for {patience} epcohs\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_layers = int(input('Conv2d with activation + Max-pool + Dropout for feature extraction = 1 feature extraction layer \\nHow many of such feature extraction layers you want to use ? '))    \n",
    "no_conv = int(input('How many conv2d layers you want to use in each feature extraction layer ? '))\n",
    "no_filters = int(input('Put no. of filters in 1st conv2d layer: '))\n",
    "size_filter = int(input('Enter size of filter (width or height): '))\n",
    "f_dropout = int(input('Enter dropout rate for feature extraction: '))/100\n",
    "\n",
    "no_d_layers = int(input('Dense with activation + Dropout for desnse layer = 1 dense layer \\nHow many of such dense layers you want to use ? '))\n",
    "d_neurons = int(input('Enter no.of neurons you want to use in 1st dense layer: '))\n",
    "d_dropout = int(input('Enter dropout rate for dense layer: '))/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_Model():        \n",
    "        \n",
    "    model = Sequential(name = 'CUSTOM')\n",
    "    \n",
    "    \n",
    "    # feature extraction\n",
    "    m, n = 0, 0 # m = increamental factor of no. of filters, # n = total no. of filters in convolution layer\n",
    "    for l in range(no_layers):\n",
    "        m = 2**l  \n",
    "        n = no_filters*m \n",
    "        for i in range(no_conv):\n",
    "            model.add(Conv2D(n, \n",
    "                             (size_filter,size_filter), \n",
    "                             kernel_regularizer=l2(lambd), \n",
    "                             bias_regularizer=l2(lambd),\n",
    "                             padding = 'same', \n",
    "                             input_shape = dim))\n",
    "            model.add(LeakyReLU())\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "        model.add(Dropout(f_dropout))\n",
    "    \n",
    "    \n",
    "    # flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    # dense layer\n",
    "    m, n = 0, 0\n",
    "    for d in range(no_d_layers):\n",
    "        m = 2**d\n",
    "        n = d_neurons//m\n",
    "        model.add(Dense(n, kernel_regularizer=l2(lambd), bias_regularizer=l2(lambd)))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(d_dropout))\n",
    "    model.add(Dense(output_neurons, output_activation))\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_no = len(os.listdir(train_directory))\n",
    "\n",
    "print(\"This is a \" + str(class_no) + \"-Class Classification\")\n",
    "\n",
    "if class_no <= 2:\n",
    "    class_mode = 'binary'\n",
    "    output_activation = 'sigmoid'\n",
    "    output_neurons = 1\n",
    "    losses = 'binary_crossentropy'\n",
    "\n",
    "else:\n",
    "    class_mode = 'categorical'\n",
    "    output_activation = 'softmax'\n",
    "    output_neurons = class_no\n",
    "    losses = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_selection():\n",
    "    print(\"\\nSelect a optimizer which will reduce the loss of the model.\\n\")\n",
    "\n",
    "    optimizer_select = int(input(\"Press 1 to select Stochastic Gradient Descent\\nPress 2 to select RMSprop\\nPress 3 to select Adagrad\\nPress 4 to select Adadelta\\nPress 5 to select Adam\\nPress 6 to select Adamax\\nPress 7 to select Nadam\\n\"))\n",
    "\n",
    "    if optimizer_select == 1:\n",
    "        optimizer = SGD(lr = learning_rate, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "    elif optimizer_select == 2:\n",
    "        optimizer = RMSprop(learning_rate, rho = 0.9)\n",
    "\n",
    "    elif optimizer_select == 3:\n",
    "        optimizer = Adagrad(learning_rate)\n",
    "\n",
    "    elif optimizer_select == 4:\n",
    "        optimizer = Adadelta(learning_rate, rho = 0.95)\n",
    "\n",
    "    elif optimizer_select == 5:\n",
    "        optimizer = Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "\n",
    "    elif optimizer_select == 6:\n",
    "        optimizer = Adamax(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n",
    "\n",
    "    elif optimizer_select == 7:\n",
    "        optimizer = Nadam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.999)\n",
    "   \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = int(input(\"Image Dimension(H or W): \"))\n",
    "w = h\n",
    "color = int(input(\"Press 1 for RGB \\nPress 2 for Grayscale\"))\n",
    "if color == 1:\n",
    "    color_mode = 'rgb'\n",
    "    dim = (h,w,3)\n",
    "elif color == 2:\n",
    "    color_mode = 'grayscale'\n",
    "    dim = (h,w,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_fold = int(input(\"Enter number of folds: \"))\n",
    "\n",
    "train_scores, val_scores, test_scores, histories = list(), list(), list(), list()\n",
    "\n",
    "kfold = KFold(n_fold, shuffle=True, random_state=1)\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "for train, val in kfold.split(x, y):\n",
    "    model = Custom_Model()\n",
    "    model.compile(optimizer = optimizer, loss = losses, metrics = ['accuracy', \n",
    "                                                               tf.keras.metrics.Precision(), \n",
    "                                                               tf.keras.metrics.Recall(), \n",
    "                                                               tf.keras.metrics.TruePositives(), \n",
    "                                                               tf.keras.metrics.TrueNegatives(), \n",
    "                                                               tf.keras.metrics.FalsePositives(),\n",
    "                                                               tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Training for fold {}...'.format(fold_no))\n",
    "\n",
    "    history = model.fit(x[train], y[train], \n",
    "                        epochs=epoch, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=(x[val], y[val]), \n",
    "                        callbacks = callback, \n",
    "                        verbose=1)\n",
    "\n",
    "    train_score = model.evaluate(x[train], y[train])[1]\n",
    "    val_score = model.evaluate(x[val], y[val])[1]\n",
    "    test_score = model.evaluate(testX_norm, testY)[1]\n",
    "\n",
    "    print(\"\\nTraining Acc: {},\\nValidation Acc: {},\\nTest Acc: {}\".format(train_score, val_score, test_score))\n",
    "\n",
    "    train_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    test_scores.append(test_score)\n",
    "    histories.append(history)\n",
    "\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "val_acc = []\n",
    "tr_acc = []\n",
    "ts_acc = []\n",
    "\n",
    "for i in range(len(val_scores)):\n",
    "    val_acc.append(val_scores[i])\n",
    "    tr_acc.append(train_scores[i])\n",
    "    ts_acc.append(test_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(histories)):\n",
    "    plt.title('Training Loss')\n",
    "    plt.plot(histories[i].history['loss'], color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(histories)):\n",
    "    plt.title('Validation Loss')\n",
    "    plt.plot(histories[i].history['val_loss'], color='orange')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(histories)):\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.plot(histories[i].history['accuracy'], color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(histories)):\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.plot(histories[i].history['val_accuracy'], color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = np.mean(tr_acc), np.std(tr_acc)\n",
    "cv_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_val = np.mean(val_acc), np.std(val_acc)\n",
    "cv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_test = np.mean(ts_acc), np.std(ts_acc)\n",
    "cv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss = []\n",
    "tr_acc = []\n",
    "tr_val_loss = []\n",
    "tr_val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 0,0\n",
    "for i in range(len(histories[i].history['loss'])):\n",
    "    tr_loss_epch = []\n",
    "    tr_acc_epch = []\n",
    "    tr_val_loss_epch = []\n",
    "    tr_val_acc_epch = []\n",
    "    for j in range(len(histories)):\n",
    "        tr_loss_epch.append((histories[j].history['loss'][i]))\n",
    "        tr_acc_epch.append((histories[j].history['accuracy'][i]))\n",
    "        tr_val_loss_epch.append((histories[j].history['val_loss'][i]))\n",
    "        tr_val_acc_epch.append((histories[j].history['val_accuracy'][i]))\n",
    "\n",
    "    tr_loss.append(np.mean(tr_loss_epch))\n",
    "    tr_acc.append(np.mean(tr_acc_epch))\n",
    "    tr_val_loss.append(np.mean(tr_val_loss_epch))\n",
    "    tr_val_acc.append(np.mean(tr_val_acc_epch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_loss)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.savefig(os.path.join(char, 'avg_tr_loss.eps'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_val_loss)\n",
    "plt.title('Validation Loss')\n",
    "plt.savefig(os.path.join(char, 'avg_val_loss.eps'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_acc)\n",
    "plt.title('Training Accuracy')\n",
    "plt.savefig(os.path.join(char, 'avg_tr_acc.eps'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_val_acc)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.savefig(os.path.join(char, 'avg_val_acc.eps'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_loss, 'r', label='Training Loss')\n",
    "plt.plot(tr_val_loss, 'b', label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Training and validation Loss vs Epochs')\n",
    "plt.legend()\n",
    "accuracy_fig_name = \"loss.eps\"\n",
    "plt.savefig(os.path.join(char, accuracy_fig_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_acc, 'r', label='Training Accuracy')\n",
    "plt.plot(tr_val_acc, 'b', label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title('Training and validation Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "accuracy_fig_name = \"acc.eps\"\n",
    "plt.savefig(os.path.join(char, accuracy_fig_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "readme_name_text = \"readme.txt\"\n",
    "print(f\"Please read the text file named {readme_name_text} for detailed information of the model.\")\n",
    "\n",
    "completeName_txt = os.path.join(char, readme_name_text) \n",
    "\n",
    "readme = open(completeName_txt, \"w\")\n",
    "\n",
    "if len(os.listdir(train_directory)) > 2:\n",
    "    readme.write(f\"This is a {len(os.listdir(train_directory))}-class CLASSIFICATION\")\n",
    "else:\n",
    "    readme.write(\"This is a BINARY CLASSIFICATION\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--HYPERPARAMETERS--\\n\")\n",
    "readme.write(f\"\\nInitial Learning Rate = {learning_rate}\")\n",
    "readme.write(f\"\\nNo. of epochs = {len(acc)}\")\n",
    "readme.write(f\"\\nBatch Size = {batch_size}\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--MODEL-PARAMETERS--\")\n",
    "readme.write(f\"\\nDropout for feature extraction = {(int(f_dropout*100))} %\")\n",
    "readme.write(f\"\\nDropout for dense layer = {(int(d_dropout*100))} %\")\n",
    "readme.write(f\"\\nOptimizer = {optimizer}\\n\\n\")\n",
    "\n",
    "\n",
    "readme.write(\"Trained on a Custom Prebuilt Model\\n\")\n",
    "readme.write(f\"\\nFilter size = {size_filter}x{size_filter}\\n\\n\")\n",
    "with redirect_stdout(readme):\n",
    "    model.summary()\n",
    "        \n",
    "    \n",
    "readme.write(\"\\n\\n--MODEL-PERFORMANCE--\")\n",
    "readme.write(f\"\\nTest Accuracy = {test_accuracy} %\")\n",
    "readme.write(f\"\\nTest Precision = {test_precision} %\")\n",
    "readme.write(f\"\\nTest Recall = {test_recall} %\")\n",
    "readme.write(f\"\\nTrue Positive = {tp}\")\n",
    "readme.write(f\"\\nTrue Negetive = {tn}\")\n",
    "readme.write(f\"\\nFalse Positive = {fp}\")\n",
    "readme.write(f\"\\nFalse Negetive = {fn}\")\n",
    "readme.write(f\"\\nSensitivity = {sensitivity}\")\n",
    "readme.write(f\"\\nSpecificity = {specificity}\\n\\n\\n\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--MODEL-CHARACTERISTICS--\")\n",
    "readme.write(f\"\\nacc = {acc}\")\n",
    "readme.write(f\"\\n\\nval_acc = {val_acc}\")\n",
    "readme.write(f\"\\n\\nloss = {loss}\")\n",
    "readme.write(f\"\\n\\nval_loss = {val_loss}\")\n",
    "\n",
    "\n",
    "readme.write(\"\\n\\n--Classification Report--\\n\")\n",
    "readme.write(classification_reports)\n",
    "\n",
    "readme.write(f\"\\nSensitivity = {sensitivity*100} %\")\n",
    "readme.write(f\"\\nSpecificity = {specificity*100} %\")\n",
    "\n",
    "\n",
    "readme.write(f\"\\nExecution Time: {duration} seconds\")\n",
    "\n",
    "readme.write(\"\\n\\nCreated using Self-Regulated Image Classifier using Convolution Neural Network\")\n",
    "\n",
    "readme.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
